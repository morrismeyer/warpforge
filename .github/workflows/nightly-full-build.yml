name: Nightly Full Build

# Full clean build and comprehensive test suite
# Runs all unit tests, integration tests, and performance benchmarks
# with baseline validation to catch regressions

on:
  # Manual trigger for testing (enable this first to verify the workflow works)
  workflow_dispatch:
    inputs:
      rebuild_pytorch_venv:
        description: 'Rebuild PyTorch venv from scratch (adds ~60 min)'
        required: false
        default: 'true'
        type: boolean
      skip_perf_tests:
        description: 'Skip performance tests (faster for debugging)'
        required: false
        default: 'false'
        type: boolean
      skip_gpu_boxes:
        description: 'Skip GPU box tests (NUC only)'
        required: false
        default: 'false'
        type: boolean

  # Nightly schedule - UNCOMMENT AFTER MANUAL TESTING CONFIRMS IT WORKS
  # schedule:
  #   - cron: '0 7 * * *'  # 2 AM EST (7 AM UTC) every day

jobs:
  nightly-full-build:
    name: Full Build & Test Suite
    runs-on: [self-hosted, linux, x64]
    timeout-minutes: 180  # 3 hour max for full suite

    env:
      # Force all tests to actually run
      GRADLE_OPTS: '-Dorg.gradle.caching=false'

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Pre-flight diagnostics
        run: |
          echo "=== Nightly Full Build Started ==="
          echo "Trigger: ${{ github.event_name }}"
          echo "Rebuild PyTorch venv: ${{ inputs.rebuild_pytorch_venv }}"
          echo "Skip perf tests: ${{ inputs.skip_perf_tests }}"
          echo "Skip GPU boxes: ${{ inputs.skip_gpu_boxes }}"
          echo "Runner: $(hostname)"
          echo "Date: $(date)"
          df -h
          free -h || true

      # ========================================
      # PHASE 1: Full Clean (Nuclear Option)
      # ========================================
      - name: Full repository clean
        run: |
          set -euo pipefail
          NUC_REPO="${HOME}/surfworks/warpforge"
          REBUILD_VENV="${{ inputs.rebuild_pytorch_venv }}"

          echo "=== Full Clean Phase ==="
          echo "Rebuild PyTorch venv: $REBUILD_VENV"

          if [[ -d "$NUC_REPO/.git" ]]; then
            cd "$NUC_REPO"
            git fetch origin
            git reset --hard "origin/${GITHUB_REF_NAME:-main}"

            # Determine what to clean
            if [[ "$REBUILD_VENV" == "true" ]]; then
              echo "NUCLEAR CLEAN: Removing EVERYTHING including PyTorch venv"
              echo "This verifies a new developer can build from scratch"
              git clean -fdx
              # Also remove any cached PyTorch source
              rm -rf snakegrinder-dist/.pytorch-venv
            else
              echo "Standard clean: Preserving PyTorch venv for speed"
              git clean -fdx -e '.pytorch-venv'
            fi

            # Explicitly remove Gradle caches
            rm -rf ~/.gradle/caches/build-cache-*
            rm -rf "$NUC_REPO/.gradle/build-cache"
            rm -rf "$NUC_REPO/.gradle"

            echo "Clean complete. Disk usage:"
            du -sh "$NUC_REPO" || true
            du -sh "$NUC_REPO/snakegrinder-dist/.pytorch-venv" 2>/dev/null || echo "No PyTorch venv"
          else
            echo "NUC repo not found, will be cloned"
            mkdir -p "$(dirname "$NUC_REPO")"
            git clone git@github.com:morrismeyer/warpforge.git "$NUC_REPO"
            cd "$NUC_REPO"
          fi

          echo "Git status after clean:"
          git status

      # ========================================
      # PHASE 1.5: Build PyTorch Venv (if needed)
      # ========================================
      - name: Build PyTorch venv from scratch
        if: ${{ inputs.rebuild_pytorch_venv == 'true' }}
        run: |
          set -euo pipefail
          cd ~/surfworks/warpforge

          echo "=== PyTorch Venv Build Phase ==="
          echo "This verifies that GraalPy + PyTorch builds correctly from source"
          echo "Expected duration: 30-60 minutes"

          # Check build dependencies
          echo "Checking build dependencies..."
          command -v cmake >/dev/null || { echo "ERROR: cmake not found"; exit 1; }
          command -v ninja >/dev/null || { echo "ERROR: ninja not found"; exit 1; }
          command -v g++ >/dev/null || { echo "ERROR: g++ not found"; exit 1; }

          echo "Dependencies OK. Starting PyTorch venv build..."
          START_TIME=$(date +%s)

          # Build the venv - this downloads GraalPy and builds PyTorch from source
          ./gradlew :snakegrinder-dist:buildPytorchVenv --no-build-cache --info 2>&1 | tee pytorch-venv-build.log

          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          echo "PyTorch venv build completed in $((DURATION / 60)) minutes $((DURATION % 60)) seconds"

          # Verify the venv was created
          VENV_DIR="snakegrinder-dist/.pytorch-venv"
          if [[ ! -d "$VENV_DIR" ]]; then
            echo "ERROR: PyTorch venv was not created at $VENV_DIR"
            exit 1
          fi

          echo "Verifying PyTorch import..."
          "$VENV_DIR/bin/graalpy" -c "import torch; print(f'PyTorch {torch.__version__} OK')"

          # Prune the venv to save space
          echo "Pruning venv to reduce size..."
          ./gradlew :snakegrinder-dist:prunePytorchVenv --no-build-cache

          echo "Final venv size:"
          du -sh "$VENV_DIR"

          echo "PyTorch venv build SUCCESS"

      # ========================================
      # PHASE 2: Full Build (No Cache)
      # ========================================
      - name: Full build - all modules
        run: |
          set -euo pipefail
          cd ~/surfworks/warpforge

          echo "=== Full Build Phase (No Cache) ==="

          # Build everything from scratch
          ./gradlew clean assemble --no-build-cache --no-configuration-cache --info 2>&1 | tee build.log

          echo "Build complete. Checking artifacts..."
          find . -name "*.jar" -path "*/build/libs/*" | head -20

      # ========================================
      # PHASE 3: Unit Tests (NUC)
      # ========================================
      - name: Unit tests - all modules
        run: |
          set -euo pipefail
          cd ~/surfworks/warpforge

          echo "=== Unit Tests Phase ==="

          # Run all unit tests without build cache
          ./gradlew test --no-build-cache --no-configuration-cache 2>&1 | tee test.log

          # Verify test counts
          echo "=== Test Result Summary ==="
          for module in snakeburger-core warpforge-core warpforge-io; do
            TEST_DIR="${module}/build/test-results/test"
            if [[ -d "$TEST_DIR" ]]; then
              COUNT=$(find "$TEST_DIR" -name "*.xml" -exec grep -h "tests=" {} \; | grep -oE 'tests="[0-9]+"' | grep -oE '[0-9]+' | awk '{sum+=$1} END {print sum}')
              echo "${module}: ${COUNT:-0} tests"
            fi
          done

      - name: Verify snakeburger-core tests
        run: |
          set -euo pipefail
          cd ~/surfworks/warpforge

          TEST_DIR="snakeburger-core/build/test-results/test"
          COUNT=$(find "$TEST_DIR" -name "*.xml" -exec grep -h "tests=" {} \; | grep -oE 'tests="[0-9]+"' | grep -oE '[0-9]+' | awk '{sum+=$1} END {print sum}')

          if [[ "${COUNT:-0}" -lt 300 ]]; then
            echo "ERROR: Expected at least 300 snakeburger-core tests, found ${COUNT:-0}"
            exit 1
          fi
          echo "SUCCESS: snakeburger-core tests passed (${COUNT} tests)"

      - name: Verify warpforge-core tests
        run: |
          set -euo pipefail
          cd ~/surfworks/warpforge

          TEST_DIR="warpforge-core/build/test-results/test"
          COUNT=$(find "$TEST_DIR" -name "*.xml" -exec grep -h "tests=" {} \; | grep -oE 'tests="[0-9]+"' | grep -oE '[0-9]+' | awk '{sum+=$1} END {print sum}')

          if [[ "${COUNT:-0}" -lt 100 ]]; then
            echo "ERROR: Expected at least 100 warpforge-core tests, found ${COUNT:-0}"
            exit 1
          fi
          echo "SUCCESS: warpforge-core tests passed (${COUNT} tests)"

      - name: Verify warpforge-io tests
        run: |
          set -euo pipefail
          cd ~/surfworks/warpforge

          TEST_DIR="warpforge-io/build/test-results/test"
          COUNT=$(find "$TEST_DIR" -name "*.xml" -exec grep -h "tests=" {} \; | grep -oE 'tests="[0-9]+"' | grep -oE '[0-9]+' | awk '{sum+=$1} END {print sum}')

          if [[ "${COUNT:-0}" -lt 150 ]]; then
            echo "ERROR: Expected at least 150 warpforge-io tests, found ${COUNT:-0}"
            exit 1
          fi
          echo "SUCCESS: warpforge-io tests passed (${COUNT} tests)"

      # ========================================
      # PHASE 4: SnakeGrinder Distribution Tests
      # ========================================
      - name: SnakeGrinder distribution tests
        run: |
          set -euo pipefail
          cd ~/surfworks/warpforge

          echo "=== SnakeGrinder Tests ==="

          VENV_DIR="snakegrinder-dist/.pytorch-venv"
          if [[ -d "$VENV_DIR" ]]; then
            echo "PyTorch venv found, running distribution tests..."
            ./gradlew :snakegrinder-dist:testDist --no-build-cache --no-configuration-cache 2>&1 | tee snakegrinder-test.log
            echo "SnakeGrinder tests passed"
          else
            echo "WARNING: PyTorch venv not found at $VENV_DIR"
            echo "Skipping snakegrinder-dist tests"
            echo "To enable, run: ./gradlew :snakegrinder-dist:buildPytorchVenv"
          fi

      # ========================================
      # PHASE 5: Native Image Build
      # ========================================
      - name: Build native-image UCC performance test
        run: |
          set -euo pipefail
          cd ~/surfworks/warpforge

          echo "=== Native Image Build ==="

          # Build native-image for UCC performance tests
          ./gradlew :warpforge-io:buildNativeUccPerfTest --no-build-cache 2>&1 | tee native-build.log || {
            echo "WARNING: Native image build failed (may be expected if GraalVM native-image not configured)"
            echo "Continuing without native tests..."
          }

          if [[ -f "warpforge-io/build/native/nativeCompile/ucc-perf-test" ]]; then
            echo "Native UCC perf test built successfully"
            ls -la warpforge-io/build/native/nativeCompile/
          fi

      # ========================================
      # PHASE 6: GPU Box Tests (NVIDIA + AMD)
      # ========================================
      - name: NVIDIA box - build and test
        if: ${{ inputs.skip_gpu_boxes != 'true' }}
        run: |
          set -euo pipefail
          cd ~/surfworks/warpforge

          echo "=== NVIDIA Box Tests ==="

          # Use the existing orchestrator but with no-build-cache
          export TEST_CMD_OVERRIDE="./gradlew test --no-build-cache"
          export NVIDIA_TEST_CMD_OVERRIDE="./gradlew nvidiaTest --no-build-cache"

          chmod +x holmes-lab/mark1/ci-scripts/orchestrate-nvidia-build.sh
          bash holmes-lab/mark1/ci-scripts/orchestrate-nvidia-build.sh 2>&1 | tee nvidia-test.log

      - name: AMD box - build and test
        if: ${{ inputs.skip_gpu_boxes != 'true' }}
        run: |
          set -euo pipefail
          cd ~/surfworks/warpforge

          echo "=== AMD Box Tests ==="

          # Use the existing orchestrator but with no-build-cache
          export TEST_CMD_OVERRIDE="./gradlew test --no-build-cache"
          export AMD_TEST_CMD_OVERRIDE="./gradlew amdTest --no-build-cache"

          chmod +x holmes-lab/mark1/ci-scripts/orchestrate-amd-build.sh
          bash holmes-lab/mark1/ci-scripts/orchestrate-amd-build.sh 2>&1 | tee amd-test.log

      # ========================================
      # PHASE 7: Two-Node Collective Performance Tests
      # ========================================
      - name: Two-node UCC collective performance tests
        if: ${{ inputs.skip_perf_tests != 'true' && inputs.skip_gpu_boxes != 'true' }}
        run: |
          set -euo pipefail
          cd ~/surfworks/warpforge

          echo "=== Two-Node Collective Performance Tests ==="
          echo "Testing NVIDIA <-> AMD Mellanox 100GbE collective operations"

          # This requires both GPU boxes to be up and coordinated
          # The orchestrator scripts should have already woken them

          NVIDIA_HOST="${NVIDIA_HOST:-nvidia}"
          AMD_HOST="${AMD_HOST:-amd}"
          MASTER_IP="${MASTER_IP:-10.0.0.1}"

          echo "NVIDIA host: $NVIDIA_HOST"
          echo "AMD host: $AMD_HOST"
          echo "Master IP: $MASTER_IP"

          # Create performance results directory
          PERF_RESULTS_DIR="$HOME/surfworks/warpforge/holmes-lab/mark1/results/nightly-$(date +%Y%m%d)"
          mkdir -p "$PERF_RESULTS_DIR"

          # Launch master (rank 0) on NVIDIA box
          echo "Starting UCC perf master on NVIDIA..."
          ssh -o ConnectTimeout=10 "$NVIDIA_HOST" "cd ~/surfworks/warpforge && \
            ./gradlew :warpforge-io:uccPerfMaster \
              -Pmaster=$MASTER_IP -Pport=29500 \
              -Psize=16777216 -Piterations=100 -Pwarmup=10 \
              --no-build-cache" > "$PERF_RESULTS_DIR/nvidia-perf.log" 2>&1 &
          NVIDIA_PID=$!

          # Give master time to start listening
          sleep 5

          # Launch worker (rank 1) on AMD box
          echo "Starting UCC perf worker on AMD..."
          ssh -o ConnectTimeout=10 "$AMD_HOST" "cd ~/surfworks/warpforge && \
            ./gradlew :warpforge-io:uccPerfWorker \
              -Pmaster=$MASTER_IP -Pport=29500 \
              -Psize=16777216 -Piterations=100 -Pwarmup=10 \
              --no-build-cache" > "$PERF_RESULTS_DIR/amd-perf.log" 2>&1 &
          AMD_PID=$!

          # Wait for both to complete
          echo "Waiting for performance tests to complete..."
          wait $NVIDIA_PID || echo "WARNING: NVIDIA perf test exited with non-zero status"
          wait $AMD_PID || echo "WARNING: AMD perf test exited with non-zero status"

          echo "=== Performance Results ==="
          cat "$PERF_RESULTS_DIR/nvidia-perf.log" || true

          # TODO: Parse results and compare against baseline
          # TODO: Fail if regression detected

          echo "Performance test logs saved to: $PERF_RESULTS_DIR"

      - name: Native-image UCC performance comparison
        if: ${{ inputs.skip_perf_tests != 'true' && inputs.skip_gpu_boxes != 'true' }}
        run: |
          set -euo pipefail
          cd ~/surfworks/warpforge

          echo "=== Native Image Performance Comparison ==="

          NATIVE_EXE="warpforge-io/build/native/nativeCompile/ucc-perf-test"
          if [[ ! -f "$NATIVE_EXE" ]]; then
            echo "WARNING: Native executable not found, skipping native perf comparison"
            exit 0
          fi

          NVIDIA_HOST="${NVIDIA_HOST:-nvidia}"
          AMD_HOST="${AMD_HOST:-amd}"
          MASTER_IP="${MASTER_IP:-10.0.0.1}"

          PERF_RESULTS_DIR="$HOME/surfworks/warpforge/holmes-lab/mark1/results/nightly-$(date +%Y%m%d)"
          mkdir -p "$PERF_RESULTS_DIR"

          # Copy native executable to both boxes
          echo "Deploying native executable to GPU boxes..."
          scp "$NATIVE_EXE" "$NVIDIA_HOST:~/surfworks/warpforge/ucc-perf-test" || true
          scp "$NATIVE_EXE" "$AMD_HOST:~/surfworks/warpforge/ucc-perf-test" || true

          # Run native perf tests (similar coordination as JVM tests)
          echo "Running native-image performance tests..."
          # TODO: Implement native perf test coordination

          echo "Native performance comparison complete"

      # ========================================
      # PHASE 8: Performance Baseline Validation
      # ========================================
      - name: Validate performance against baselines
        if: ${{ inputs.skip_perf_tests != 'true' }}
        run: |
          set -euo pipefail
          cd ~/surfworks/warpforge

          echo "=== Performance Baseline Validation ==="

          BASELINE_DIR="holmes-lab/mark1/baselines"
          RESULTS_DIR="$HOME/surfworks/warpforge/holmes-lab/mark1/results/nightly-$(date +%Y%m%d)"

          if [[ ! -d "$BASELINE_DIR" ]]; then
            echo "No baseline directory found at $BASELINE_DIR"
            echo "Creating baseline directory for future runs..."
            mkdir -p "$BASELINE_DIR"
            echo "First run - no baseline comparison available"
            exit 0
          fi

          # TODO: Implement baseline comparison logic
          # - Parse performance results from logs
          # - Compare against stored baselines
          # - Fail if regression > threshold (e.g., 10%)
          # - Update baselines if improvement detected

          echo "Baseline validation complete (TODO: implement comparison logic)"

      # ========================================
      # Summary
      # ========================================
      - name: Test summary
        if: always()
        run: |
          echo "========================================"
          echo "       NIGHTLY FULL BUILD SUMMARY       "
          echo "========================================"
          echo "Completed at: $(date)"
          echo ""
          echo "Test Results:"
          cd ~/surfworks/warpforge

          for module in snakeburger-core warpforge-core warpforge-io snakeburger-codegen; do
            TEST_DIR="${module}/build/test-results/test"
            if [[ -d "$TEST_DIR" ]]; then
              TOTAL=$(find "$TEST_DIR" -name "*.xml" -exec grep -h "tests=" {} \; | grep -oE 'tests="[0-9]+"' | grep -oE '[0-9]+' | awk '{sum+=$1} END {print sum}')
              FAILED=$(find "$TEST_DIR" -name "*.xml" -exec grep -h "failures=" {} \; | grep -oE 'failures="[0-9]+"' | grep -oE '[0-9]+' | awk '{sum+=$1} END {print sum}')
              echo "  ${module}: ${TOTAL:-0} tests, ${FAILED:-0} failures"
            fi
          done

          echo ""
          echo "========================================"
