# WarpForge CUDA Container
# NVIDIA GPU-enabled image for CUDA workloads
#
# Build:
#   docker build -f Dockerfile.cuda -t warpforge:0.1.0-cuda ..
#
# Run (requires nvidia-container-toolkit):
#   docker run --rm --gpus all warpforge:0.1.0-cuda gpu-info
#   docker run --rm --gpus all -v /data:/data warpforge:0.1.0-cuda submit --source /data/model.py

ARG WARPFORGE_VERSION=0.1.0
ARG CUDA_VERSION=12.4.0
ARG UBUNTU_VERSION=22.04

# Use NVIDIA CUDA base image
FROM nvidia/cuda:${CUDA_VERSION}-runtime-ubuntu${UBUNTU_VERSION}

# Install runtime dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    ca-certificates \
    libgomp1 \
    libstdc++6 \
    libc6 \
    # RDMA support for multi-node training
    libibverbs1 \
    librdmacm1 \
    && rm -rf /var/lib/apt/lists/*

# Copy WarpForge distribution
ARG WARPFORGE_VERSION
COPY build/warpforge-${WARPFORGE_VERSION} /opt/warpforge

# Set up environment
ENV WARPFORGE_HOME=/opt/warpforge
ENV PATH="/opt/warpforge/bin:${PATH}"
ENV PYTORCH_VENV="/opt/warpforge/lib/python"

# CUDA environment
ENV CUDA_HOME=/usr/local/cuda
ENV LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}"

# UCX/UCC environment for collective operations (if bundled)
ENV UCX_TLS=rc,ud,sm,self
ENV UCX_NET_DEVICES=mlx5_0:1

# Create non-root user
RUN useradd -m -s /bin/bash warpforge && \
    chown -R warpforge:warpforge /opt/warpforge

USER warpforge
WORKDIR /home/warpforge

# Default entrypoint
ENTRYPOINT ["/opt/warpforge/bin/warpforge"]
CMD ["--help"]

# Health check - verify GPU access
HEALTHCHECK --interval=60s --timeout=30s --start-period=10s --retries=3 \
    CMD ["/opt/warpforge/bin/warpforge", "gpu-info"]

# Labels
LABEL org.opencontainers.image.title="WarpForge CUDA"
LABEL org.opencontainers.image.description="WarpForge ML Compiler - NVIDIA CUDA GPU image"
LABEL org.opencontainers.image.version="${WARPFORGE_VERSION}"
LABEL com.nvidia.volumes.needed="nvidia_driver"
