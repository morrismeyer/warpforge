{
  "version": "1.0",
  "description": "E2E test fixtures manifest - large fixtures are downloaded on demand",
  "base_url": "https://warpforge-fixtures.s3.amazonaws.com/e2e",
  "fixtures": {
    "bert_squad_mini": {
      "size_mb": 0.5,
      "download": true,
      "sha256": null,
      "description": "Minimal BERT fixture for quick validation"
    },
    "bert_squad_small": {
      "size_mb": 43,
      "download": true,
      "sha256": null,
      "description": "Small BERT-Small SQuAD fixture"
    },
    "bert_squad_base": {
      "size_mb": 328,
      "download": true,
      "sha256": null,
      "description": "Full BERT-Base SQuAD fixture for comprehensive testing"
    },
    "add": {
      "size_mb": 0.01,
      "download": false,
      "description": "Simple add operation - small enough to commit"
    },
    "subtract": {
      "size_mb": 0.01,
      "download": false,
      "description": "Simple subtract operation"
    },
    "multiply": {
      "size_mb": 0.01,
      "download": false,
      "description": "Simple multiply operation"
    },
    "relu": {
      "size_mb": 0.01,
      "download": false,
      "description": "ReLU activation"
    },
    "sigmoid": {
      "size_mb": 0.01,
      "download": false,
      "description": "Sigmoid activation"
    },
    "tanh": {
      "size_mb": 0.01,
      "download": false,
      "description": "Tanh activation"
    },
    "softmax": {
      "size_mb": 0.01,
      "download": false,
      "description": "Softmax operation"
    },
    "softmax_3d": {
      "size_mb": 0.01,
      "download": false,
      "description": "3D softmax operation"
    },
    "gelu": {
      "size_mb": 0.01,
      "download": false,
      "description": "GELU activation"
    },
    "gelu_tanh": {
      "size_mb": 0.01,
      "download": false,
      "description": "GELU with tanh approximation"
    },
    "silu": {
      "size_mb": 0.01,
      "download": false,
      "description": "SiLU activation"
    },
    "exp": {
      "size_mb": 0.01,
      "download": false,
      "description": "Exponential operation"
    },
    "abs": {
      "size_mb": 0.01,
      "download": false,
      "description": "Absolute value operation"
    },
    "negate": {
      "size_mb": 0.01,
      "download": false,
      "description": "Negation operation"
    },
    "linear": {
      "size_mb": 0.1,
      "download": false,
      "description": "Linear layer with bias"
    },
    "linear_no_bias": {
      "size_mb": 0.1,
      "download": false,
      "description": "Linear layer without bias"
    },
    "embedding": {
      "size_mb": 0.1,
      "download": false,
      "description": "Embedding lookup"
    },
    "embedding_with_position": {
      "size_mb": 0.1,
      "download": false,
      "description": "Embedding with positional encoding"
    },
    "layer_norm": {
      "size_mb": 0.01,
      "download": false,
      "description": "Layer normalization"
    },
    "layer_norm_3d": {
      "size_mb": 0.01,
      "download": false,
      "description": "3D layer normalization"
    },
    "multi_head_attention": {
      "size_mb": 0.5,
      "download": false,
      "description": "Multi-head attention mechanism"
    },
    "scaled_dot_product_attention": {
      "size_mb": 0.1,
      "download": false,
      "description": "Scaled dot-product attention"
    },
    "ffn_block": {
      "size_mb": 0.2,
      "download": false,
      "description": "Feed-forward network block"
    },
    "pre_norm_residual": {
      "size_mb": 0.1,
      "download": false,
      "description": "Pre-norm residual connection"
    },
    "transformer_encoder_block": {
      "size_mb": 1.0,
      "download": false,
      "description": "Full transformer encoder block"
    }
  },
  "notes": [
    "Fixtures with download=true are NOT committed to the repository",
    "Run './gradlew :warpforge-core:downloadFixtures' to fetch them",
    "Small fixtures (< 1MB) are committed directly",
    "Large fixtures (BERT, GPT-2, ViT) must be downloaded"
  ]
}
